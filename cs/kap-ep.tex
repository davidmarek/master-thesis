\chapter{Učení parametrů}

\section{Grafický model}

Máme vybraný faktor $f$, tento faktor je spojený s několika proměnnými
$\vec{x} = (x_0, x_1, \dots, x_{N_x})$
a množinami parametrů
$\vec{\Theta} = (\vec{\theta}_1, \dots, \vec{\theta}_{N_\theta})$.
Tento faktor reprezentuje podmíněnou pravděpodobnost:
$$f(\vec{x}, \Theta) = p(x_0 | x_1, \dots, x_{N_x}; \Theta)$$
Rodičovské proměnné $x_1, \dots, x_{N_x}$ označujeme jako $\vec{x^\prime}$.
Vektor $\vec{x^\prime}$ určuje, která množina parametrů bude použita.
Protože množiny parametrů jsou číslovány $1, \dots, N_\theta$ a rodičovské
proměnné $1, \dots, N_x$, musí být pro vybrání správné množiny parametrů
použito mapování $\rho(\vec{x^\prime})$.
Faktor pak může být zapsán zkráceně:
$$f(\vec{x}, \Theta) = p(x_0 | x_1, \dots, x_{N_x}; \Theta) =
\theta_{\rho(x^\prime), x_0}$$

\section{Výpočet marginálních pravděpodobností}

Pro výpočet sdružené pravděpodobnosti používáme plně faktorizovanou distribuci.
Pro každou proměnnou anebo množinu parametrů je její marginální pravděpodobnost
rovna součinu zpráv, přicházejících z faktorů, které jsou s danou proměnnou
nebo množinu parametrů propojeny.
Pro daný faktor je cavity distribuce $q^\backslash(x_i)$, popř.
$q^\backslash(\vec{\theta}_i)$ rovna součinu všech ostatních faktorů.
Aproximovaná marginální pravděpodobnost proměnné je pak součinem cavity
distribuce a zprávy z faktoru:
$$q(x_i) = q^\backslash(x_i) f(x_i)$$
$$q(\vec{\theta}_i) = q^\backslash(\vec{\theta}_i) f(\vec{\theta}_i)$$

\subsection{Marginální pravděpodobnost proměnných}

Pokud chceme aktualizovat hodnotu naší aproximace marginální pravděpodobnosti,
tak je třeba minimalizovat její vzdálenost od skutečné marginální
pravděpodobnosti:
\begin{align}
p^*(\tilde{x}_j) & = \sum_{\vec{x}: x_j = \tilde{x}_j} \int_{\vec{\Theta}}
    \prod_i q^\backslash(x_i) \prod_l q^\backslash(\vec{\theta}_l) f(\vec{x};
    \vec{\Theta}) \\
& = \sum_{\vec{x}: x_j = \tilde{x}_j} \prod_i q^\backslash(x_i)
    \int_{\vec{\theta_{\rho(\vec{x^\prime})}}}
    q^\backslash(\vec{\theta}_{\rho(\vec{x^\prime})})
    \theta_{\rho(\vec{x^\prime}), x_0} \\
& = \sum_{\vec{x}: x_j = \tilde{x}_j} \prod_i q^\backslash(x_i)
    \mathbb{E}_{q^\backslash} (\theta_{\rho(\vec{x^\prime}), x_0})
\end{align}

První řádek vychází z definice výpočtu marginální pravděpodobnosti ze sdružené
pravděpodobnosti.
Na druhém řádku byla použita definice faktoru, z integrálu byly vytaženy členy,
které neobsahují $\Theta$ a nakonec bylo využito toho, že pro množiny
parametrů, které nejsou spojeny s faktorem $f$, je jejich jejich cavity
distribuce rovná marginální distribuci a tedy $\int_{\theta_i} q(\theta_i) =
1$. Na posledním řádku byla použita definice očekávané hodnoty.

Marginální pravděpodobnost proměnné $x_i$ tedy je
\begin{equation}
p^*(\tilde x) =
    \sum_{\vec{x}: x_j = \tilde{x}_j}
        \prod_i
            q^\backslash(x_i)
            \mathbb{E}_{q^\backslash}
                (\theta_{\rho(\vec{x^\prime}), x_0})
\end{equation}

Tady docházíme k výsledku, který je velmi podobný výpočtu marginální
pravděpodobnosti v Loopy Belief Propagation algoritmu, střední hodnota
$\mathbb{E}_{q^\backslash} (\theta_{\rho(\vec{x^\prime}), x_0})$ zde
reprezentuje zprávu z vrcholu $\vec{\theta}_{\rho_x}$.

    \subsection{Marginální pravděpodobnost množin parametrů}

Pro množiny parametrů se jejich marginální pravděpodobnost spočítá podobně jako
pro proměnné.
\begin{align}
p^*(\tilde{\vec{\theta}}_j) & = \sum_{\vec{x}} \int_{\vec{\Theta}:
    \vec{\theta}_j = \tilde{\vec{\theta}}_j} \prod_i q^\backslash(x_i) \prod_l
    q^\backslash(\vec{\theta}_l) f(\vec{x}; \vec{\Theta}) \label{eq:ep:theta_1}
\\
& = \sum_{l \ne j} \sum_{\vec{x}: \rho(\vec{x^\prime}) = l} \prod_i
    q^\backslash(x_i) \int_{\vec{\Theta}: \vec{\theta}_j =
    \tilde{\vec{\theta}}_j} \prod_k q^\backslash(\vec{\theta}_k) \theta_{l,
    x_0} + \label{eq:ep:theta_2}
\\
&   + \sum_{\vec{x}: \rho(\vec{x^\prime}) = j} \prod_i q^\backslash(x_i)
    \int_{\vec{\Theta}: \vec{\theta}_j = \tilde{\vec{\theta}}_j} \prod_k
    q^\backslash(\vec{\theta}_k) \tilde{\theta}_{j, x_0}
\nonumber
\\
& = \left[ \sum_{l \ne j} \sum_{\vec{x}: \rho(\vec{x^\prime}) = l} \prod_i
    q^\backslash(x_i) \mathbb{E}_{q^\backslash(\vec{\theta}_l)} (\theta_{l,
    x_0}) \right] q^\backslash(\tilde{\vec{\theta}}_j) + \label{eq:ep:theta_3}
\\
&   + \sum_{\vec{x}: \rho(\vec{x^\prime}) = j} \prod_i q^\backslash(x_i)
    \tilde{\theta}_{j,x_0} q^\backslash(\tilde{\vec{\theta}}_j)
\label{eq:ep:theta_4}
\nonumber
\\
& = w_0 q^\backslash(\tilde{\vec{\theta}}_j) + \sum_k w_k
    \tilde{\theta}_{j,k} q^\backslash(\tilde{\vec{\theta}}_j),
\end{align}

kde

\begin{align}
w_0 & = \sum_{l \ne j} \sum_{\vec{x}: \rho(\vec{x^\prime}) = l} \prod_i
    q^\backslash(x_i) \mathbb{E}_{q^\backslash(\vec{\theta}_l)} (\theta_{l,
    x_0}) \\
w_k & = \sum_{\vec{x}: \rho(\vec{x^\prime}) = j, x_0 = k} \prod_i
    q^\backslash(x_i)
\end{align}

Opět vycházíme z výpočtu marginální pravděpodobnosti ze sdružené
pravděpodobnosti. V rovnici \eqref{eq:ep:theta_2} jsme rozdělili sumu přes
$\vec{x}$ na ty, pro které se ve faktoru použije množina parametrů
$\tilde{\vec{\theta}}_j$ a na ty ostatní. Také jsme z integrálu vytknuly součin
cavity distribucí pro proměnné. V dalším kroku \eqref{eq:ep:theta_3} jsme opět
použili toho, že integrál přes $\vec{\Theta}$ je ve skutečnosti několik
integrálů přes jednotlivé množiny parametrů. A tedy je můžeme vložit mezi
jednotlivé členy produktu cavity distribucí pro množiny parametrů. Ve výsledku
získáme $q^\backslash(\tilde{\vec{\theta}}_j) \int_{\vec{\theta}_l}
q^\backslash(\vec{\theta}_l) \theta_{l, x_0}$ a pak zbylé členy, které zmizí.

Opět tedy docházíme k vyjádření skutečné marginální pravděpodobnosti, ve které
není třeba integrovat přes všechny množiny parametrů, ale stačí jen očekávaná
hodnota těchto parametrů.

Stále tu ovšem zůstává problém, že spočítat aproximující distribuci
$q(\vec{\theta}_j)$ může být příliš složité a tak je třeba model dále
aproximovat. Pro zjednodušení výpočtu jsou zprávy z faktoru do množiny
parametrů $\hat{f}(\vec{\theta}_i)$ ve tvaru Dirichletovského rozdělení s
parametry $\hat{\vec{\alpha}}_i$:
\begin{equation}
    \hat{f}(\vec{\theta}_i) = Dir(\vec{\theta}_i; \hat{\vec{\alpha}}_i) =
    \frac{\Gamma (\sum_j \hat{\alpha}_{i,j})}{\prod_j
    \Gamma(\hat{\alpha}_{i,j})} \prod_j \theta_{i,j}^{\hat\alpha_{i,j} - 1}
\end{equation}
kde $\Gamma$ je Gamma funkce (zobecnění faktoriálu):
\begin{equation}
    \Gamma(z) = \int_0^\infty \! t^{z-1} \exp(-t) \mathrm{d}t
\end{equation}

Dirichletovské rozdělení bylo zvoleno, protože má důležité vlastnosti pro
součin, které budou využity dále pro výpočet cavity distribuce a celkové
aproximace. Pokud označíme aproximované faktory indexem $\beta$ a každý bude
mít vlastní parametry $\hat\alpha_{\beta, i}$, tak výsledná aproximace bude
tvaru:
\begin{align}
q(\vec{\theta}_i) &\propto \prod_\beta \hat{f}_{\beta}(\vec{\theta}_i) \\
&\propto \prod_\beta \prod_j \theta_{i,j}^{\hat\alpha_{\beta,i,j} - 1} \\
&\propto Dir(\vec{\theta}_i; \sum_\beta \hat{\vec{\alpha}}_{\beta,i} -
    (|\beta| - 1) \vec{1}) \\
&= Dir(\vec{\theta}_i; \vec{\alpha}_i) \label{eq:ep:aprx_1}
\end{align}
kde $\vec{\alpha}_i = \sum_\beta \hat{\vec{\alpha}}_{\beta,i} - (|\beta| - 1)
\vec{1})$.

Když tedy aktualizujeme faktor $\tilde\beta$, tak cavity distribuce bude dána:
\begin{align}
q^{\backslash \tilde\beta}(\vec{\theta_i}) &\propto \prod_{\beta \ne
    \tilde\beta} \hat{f}_{\beta}(\vec{\theta}_i) \\
& \propto Dir(\vec{\theta}_i; \vec{\alpha}_i - \hat{\vec{\alpha}}_{\beta, i} +
    \vec{1})
\end{align}

Naším cílem je nalézt parametry $\vec{\alpha}^*$, které nejvíce přiblíží
aproximovanou marginální pravděpodobnost \eqref{eq:ep:aprx_1} skutečné
marginální pravděpodobnosti \eqref{eq:ep:theta_4}. Pro výpočet vzdálenosti
použijeme Kullback-Leiblerovu divergenci:
\begin{equation}
KL(p \| q) = \int_{-\infty}^{\infty} \log\left(\frac{p(x)}{q(x)}\right)p(x)
    \mathrm{d}x
\end{equation}
Pro nalezení minima použijeme algoritmus Expectation Propagation a budeme tedy
minimalizovat $KL(p^*\| q)$.

Pokud se podíváme na skutečnou marginální pravděpodobnost
$p^*(\vec{\theta}_i)$, zjistíme, že můžeme některé její členy upravit.
Využijeme také vlastnosti gamma funkce $\Gamma(x) = (z-1) \Gamma(z-1)$.

\begin{align}
w_j \theta_j Dir(\vec{\theta}; \vec{\alpha}) &\propto
    w_j \theta_j \frac{\Gamma (\sum_i \alpha_{i})}{\prod_i \Gamma(\alpha_i)}
    \prod_i \theta_{i}^{\alpha_{i} - 1} \\
&\propto w_j \frac{\Gamma (\sum_i \alpha_{i})} {\prod_i
    \Gamma(\alpha_{i})} \theta_j^{\alpha_j} \prod_{i \ne j}
    \theta_{i}^{\alpha_{i} - 1} \\
&\propto w_j
    \frac{\Gamma (\sum_i \alpha_{i})}
         {\prod_i \Gamma(\alpha_{i})}
    \frac{\Gamma(\alpha_{j} + 1) \prod_{i \ne j} \Gamma(\alpha_i)}
         {\Gamma (1 + \sum_i \alpha_i)}
    Dir(\vec\theta; \vec{\alpha} + \vec{\delta}_j) \\
&\propto w_j
    \frac{\Gamma (\sum_i \alpha_{i})}
         {\prod_i \Gamma(\alpha_{i})}
    \frac{\alpha_j \Gamma(\alpha_j) \prod_{i \ne j} \Gamma(\alpha_i)}
         {(\sum_i \alpha_i) \Gamma (\sum_i \alpha_i)}
    Dir(\vec\theta; \vec{\alpha} + \vec{\delta}_j) \\
&\propto w_j
    \frac{\alpha_j}
         {\sum_i \alpha_i}
    Dir(\vec\theta; \vec{\alpha} + \vec{\delta}_j) \\
\end{align}

Díky této úpravě lze $p^*$ vyjádřit jako směs dvou Dirichletovských rozdělení.
\begin{equation}
p^*(\vec{\theta}) =
    w_0^* Dir(\vec{\theta}; \vec{\alpha}) +
    \sum_j w^*_j
        Dir(\vec{\theta}; \vec{\alpha} + \vec{\delta}_j)
\end{equation}
kde 
\begin{align}
    w^*_0 &\propto w_0 \\
    w^*_j &\propto w_j \frac{\alpha_j}{\sum_i \alpha_i}
\end{align}

Pro nalezení rozdělení z exponencíální rodiny tak, aby odpovídalo jinému
rozdělení, nám stačí nalézt parametry, pro které budou mít daná rozdělení
stejné momenty. V tomto případě budeme používat pouze první dva momenty a
zbytek zanedbáme. Je tedy třeba nalézt střední hodnotu a rozptyl
proměmnných z $p^*(\vec\theta)$.

\begin{align}
\mathbb{E}[aX+bY] &= a\mathbb{E}[X] + b\mathbb{E}[Y]
\\
\mathbb{E}[p^*(\vec{\theta})] &= \mathbb{E}
    \left[
        w_0^* Dir(\vec{\theta}; \vec{\alpha}) +
        \sum_j w^*_j
            Dir(\vec{\theta}; \vec{\alpha} + \vec{\delta}_j)
    \right] 
\\
&= w_0^* \mathbb{E}[Dir(\vec\theta; \vec\alpha)] +
    \sum_j w_j^* \mathbb{E}[Dir(\vec\theta; \vec\alpha + \vec{\delta}_j)]
\\
Var[p^*(\vec\theta)] &= Var
    \left[
        w_0^* Dir(\vec{\theta}; \vec{\alpha}) +
        \sum_j w^*_j
            Dir(\vec{\theta}; \vec{\alpha} + \vec{\delta}_j)
    \right]
\\
&= (w_0^*)^2 Var[Dir(\vec\theta; \vec\alpha)] + \sum_j (w_j^*)^2
Var[Dir(\vec\theta; \vec\alpha + \vec{\delta}_j)]
\end{align}

Pro náhodnou proměnnou $X$ z Dirichletovského rozdělení s parametry
$\vec\alpha$ platí

\begin{align}
\mathbb{E}[X_i] &= \frac{\alpha_i}{\sum_j \alpha_j}
\\
Var[X_i] &= \mathbb{E}[X_i^2] - \mathbb{E}[X_i]^2
\\
&= \frac{\alpha_i (\sum_j \alpha_j - \alpha_i)}
                 {(\sum_j \alpha_j)^2 (\sum_j \alpha_j + 1)}
\\
\mathbb{E}[X_i^2] &= Var[X_i] + \mathbb{E}[X_i]^2
\\
&= \mathbb{E}[X_i] \frac{1 + \alpha_i}{1 + \sum_j
    \alpha_j}
\\
\sum_j \alpha_j &= \frac{\mathbb{E}[X_1] - \mathbb{E}[X_1^2]}
                        {\mathbb{E}[X_1^2] - \mathbb{E}[X_1]^2}
\end{align}
