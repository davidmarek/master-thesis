% !TEX root = prace.tex
\chapter{Bayesovské sítě a inference}

V této kapitole představíme Bayesovské sítě, grafický model pro efektivní reprezentaci pravděpodobnostních rozdělení a nezávislostí mezi náhodnými proměnnými.
Bayesovská síť zároveň vytváří koncept pro inferenci, tedy zodpovídání dotazů nad proměnnými v síti.
Ukážeme si několik přístupů k inferenci, nejprve naivní výpočet vycházející přímo z definice.
Následně využijeme vlastností sítě a konceptů dynamického programování pro jeho zlepšení.
Analýzou složitosti exaktní inference dojdeme k závěru, že pro větší a komplexnější modely bude třeba se uchýlit k aproximacím.

Nejprve aproximujeme sdruženou pravděpodobnostní distribuci součinem marginálních distribucí a představíme Loopy Belief Propagation algoritmus.
Stále můžeme použít pouze diskrétní pravděpodobnostní distribuce se známými parametry.
Pro učení parametrů lze použít Expectation Maximization metodu, pro dialogové systémy je ovšem těžké získat dostatek učících dat.
Nakonec se dostaneme k metodě Expectation Propagation, která je zobecněním LBP a je tedy možné ji použít pro libovolné rozdělení.
Díky ní budeme schopní vytvořit generativní model pro aktualizaci dialogového stavu, který bude pracovat stejně jako LBP, ale bude schopen adaptace.

\section{Bayesovské sítě}

Bayesovské sítě jsou pravděpodobnostní grafický model, který využívá podmíněných nezávislostí pro úspornou reprezentaci sdružené pravděpodobnosti.
Bayesovská sít je orientovaný acyklický graf, jeho vrcholy jsou náhodné proměnné a hrany odpovídají přímé závislosti jednoho uzlu na druhý.
Pro každou náhodnou proměnnou v síti platí, že její pravděpodobnost je jednoznačně určena jejími rodiči v grafu.
Podmíněná pravděpodobnostní distribuce (CPD) proměnné $X$ popisuje pravděpodobnost proměnné $X$ dáno její rodiče, $P(X \mid parents(X))$.
Pokud proměnná nemá žádná rodiče, pak její podmíněná pravděpodobnostní distribuce je ekvivalentní marginální pravděpodobnostní distribuci.

Příklad Student~\cite{koller2009probabilistic}: firma zvažuje, zda-li přijme studenta.
Firma chce přijímat chytré studenty, ale nesmí je testovat na inteligenci (I) přímo.
Má však výsledek studentových SAT testů, které ale nemusí stačit pro správné zhodnocení inteligence.
Požadují tak tedy i doporučení (D) od jednoho z učitelů.
Učitel studentovi napíše doporučující dopis na základě známky (Z), kterou student získal v jeho předmětu.
Předměty se ovšem liší v obtížnosti (O) a tak je studentova známka v předmětu závislá nejen na jeho inteligenci, ale také na obtížnosti předmětu.
Grafický model reprezentující tento problém je vyobrazen na obrázku~\ref{fig:student}.
\begin{figure}
\begin{center}
\begin{tikzpicture}

\matrix[row sep=0.75cm, column sep=1.2cm]
{
    \node[latent]       (O)     {O};
    && \node[latent]    (I)     {I};
    \\
    &\node[latent]      (Z)     {Z};
    &&\node[latent]     (S)     {SAT};
    \\
    &\node[latent]      (D)     {D};
    \\
};

\edge{O}{Z}
\edge{I}{Z}
\edge{I}{S}
\edge{Z}{D}

\end{tikzpicture}
\end{center}
\label{fig:student}
\caption{Bayesovská síť pro příklad se studentem.}
\end{figure}

V tomto modelu je několik nezávislostí. Obtížnost předmětu a inteligence studenta jsou zjevně nezávislé.
Studentova známka z předmětu je závislá na obtížnosti předmětu a inteligenci studenta, ale je podmíněně nezávislá na jeho výsledku ze SAT, dáno studentova inteligence.
Konečně doporučení, které student obdrží, je podmíněně nezávislé na všech ostatních proměnných, dáno studentova známka.

Sdruženou nezávislot tohoto modelu lze zapsat ve formě podmíněných pravděpodobnostních distribucí s pomocí řetízkového pravidla.
\begin{equation}
P(O, I, Z, S, D) = P(D \mid Z) P(Z \mid O, I) P(SAT \mid I) P(O) P(I)
\end{equation}

Předpokládejme, že obtížnost předmětu, inteligence studenta, doporučující dopis a výsledek SAT jsou binární proměnné.
Známka z předmětu pak je ternární proměnná.
Pokud bychom zapsali sdruženou pravděpodobnost ve formě tabulky, tak se dostaneme k 48 položkám.
Díky rozdělení do podmíněných pravděpodobnostních rozložení, které nám bayesovská síť poskytuje, se dostáváme k $2 + 2 + 12 + 4 + 6 = 26$ položkám.
Tedy i v tomto jednoduchém modelu dochází k značné úspoře.

\section{Inference v Bayesovských sítích}

Bayesovské sítě reprezentují pravděpodobnostní model a umožňují nám nad ním provádět dotazy.
Můžeme se například ptát na marginální pravděpodobnost jednotlivých proměnných. 
Tu získáme marginalizací sdružené pravděpodobnosti, pokud vezmeme příklad se studentem a budeme chtít znát marginální pravděpodobnost známek, musíme vysčítat všechny ostatní proměnné.

\begin{equation}
P(Z) = \sum_{O, I, S, D} P(D \mid Z) P(Z \mid O, I) P(SAT \mid I) P(O) P(I)
\end{equation}

Další a asi nejčastější dotaz nastává, pokud některé náhodné proměnné pozorujeme. 
Pak chceme znát pravděpodobnost jiných proměnných dáno naše pozorování, $P(\vec{X} \mid \vec{E} = \vec{e})$, kde $\vec{X}$ jsou dotazované proměnné, $\vec{E}$ jsou pozorované proměnné a $\vec{e}$ jsou pozorované hodnoty.
Z definice podmíněné pravděpodobnosti dostáváme

\begin{equation}
P(\vec{X} \mid \vec{E} = \vec{e}) = \frac{P(\vec{X}, \vec{E} = \vec{e})}{P(\vec{E} = \vec{e})}
\end{equation}

Každou instanci jmenovatele $P(\vec{X}, \vec{E} = \vec{e})$ jde spočítat sumou sdružených pravděpodobností s ohodnoceními proměnných, které jsou kompatibilní s pozorováním a aktuální instancí. 
Pokud počítáme instanci $P(\vec{X} = \vec{x}, \vec{E} = \vec{e})$, pak získáme výsledek marginalizací všech proměnných, kromě $\vec{X}$ a $\vec{E}$, které jsou fixovány.
Pokud tedy množinu všech proměnných bez $\vec{X}$ a $\vec{E}$ označíme $\mathcal{W} = \mathcal{X} - \vec{X} - \vec{E}$, pak pravděpodobnost dané instance je

\begin{equation}
P(\vec{X} = \vec{x}, \vec{E} = \vec{e}) = \sum_{\vec{w}} P(\vec{x}, \vec{e}, \vec{w})
\end{equation}

Pro výpočet normalizační konstanty $P(\vec{E})$ musíme opět marginalizovat sdruženou pravděpodobnost, anebo si můžeme povšimnout, že platí
\begin{equation}
P(\vec{E} = \vec{e}) = \sum_{\vec{x}} P(\vec{x}, \vec{e})
\end{equation}
a tedy můžeme použít už vypočítané hodnoty.

\subsection{Exaktní inference}

V předchozí části jsme viděli, že pomocí definice podmíněné pravděpodobnosti a marginalizace sdružené pravděpodobnosti lze najít odpověď na libovolný dotaz. 
Nyní si ukážeme algoritmus, který využívá struktury Bayesovské sítě pro inferenci a navíc díky metodám dynamického programování umožňuje samotný výpočet urychlit.
Nakonec ovšem zjistíme, že pro velké sítě, které nás většinou zajímají nejvíce, nám přesná inference nebude stačit a musíme se uchýlit k aproximacím.

Začneme s inferencí v jednoduchém modelu $A \rightarrow B \rightarrow C \rightarrow D$.
Sdružená pravděpodobnost $P(A, B, C, D)$ je součinem jednotlivých podmíněných pravděpodobnostních distribucí
\begin{equation}
P(A, B, C, D) = P(D \mid C) P(C \mid B) P(B \mid A) P(A)
\end{equation}

Pokud nyní budeme chtít spočítat marginální distribuci $D$, tak musíme marginalizovat všechny ostatní proměnné
\begin{equation}
P(D) = \sum_{A, B, C} P(D \mid C) P(C \mid B) P(B \mid A) P(A)
\end{equation}

Můžeme si povšimnout, že spousta členů se bude počítat vícekrát.
Využitím metod dynamického programování a přeuspořádáním sum si můžeme mezivýsledky uložit a použít vícekrát.
\begin{equation}
P(D) = \sum_C P(D \mid C) \sum_B P(C \mid B) \sum_A P(B \mid A) P(A)
\end{equation}

Při výpočtu pak nejprve spočítáme $\psi_1(A, B) =  P(B \mid A) P(A)$,  pak vysčítáme proměnnou $A$ a získáme $\tau_1(B) = \sum_A \psi_1(A, B)$.
Pokračujeme obdobně
\begin{align}
    \psi_2(B, C) &= P(C \mid B) \tau_1(B) \\
    \tau_2(C) &= \sum_B \psi_2(B, C)
\end{align}
A nakonec spočítáme finální marginální pravděpodobnost
\begin{align}
    \psi_3(D, C) &= P(D \mid C) \tau_2(C) \\
    P(D) &= \sum_C \psi_3(D, C)
\end{align}

\begin{definice}
Nechť $\mathcal{X}$ je množina náhodných proměnných. 
Potom definujeme faktor $\phi$ jako zobrazení z $Val(\mathcal{X})$ do $\mathbb{R}$. Faktor je nezáporný, pokud všechny jeho obrazy jsou nezáporné. Množina proměnných $\mathcal{X}$ je doménou faktoru a značíme ji jako $Dom(\phi)$.
\end{definice}

Faktor, jehož doménu tvoří diskrétní proměnné, si můžeme představit jako tabulku, která obsahuje jednu hodnotu pro každé možné ohodnocení proměnných z domény.

\begin{definice}
Nechť $\vec{X}, \vec{Y}, \vec{Z}$ jsou tři disjunktní množiny náhodných proměnných.
Nechť $\phi_1(\vec{X}, \vec{Y})$ a $\phi_2(\vec{Y}, \vec{Z})$ jsou faktory.
Definujeme součin faktorů $\phi_1 \times \phi_2$ jako faktor $\psi: Val(\vec{X}, \vec{Y}, \vec{Z}) \rightarrow \mathbb{R}$ následovně:
\begin{equation*}
\psi(\vec{X}, \vec{Y}, \vec{Z}) = \phi_1(\vec{X}, \vec{Y}) \cdot \phi_2(\vec{Y}, \vec{Z})
\end{equation*}
\end{definice}

Násobíme prvky, které mají stejné ohodnocení společných proměnných $\vec{Y}$.
Stejný princip použijeme pro všechny matematické operace.

\begin{definice}
Nechť $\vec{X}$ je množina náhodných proměnných a $Y \not\in \vec{X}$ náhodná proměnná. 
Nechť $\phi(\vec{X}, Y)$ je faktor.
Definujeme marginalizaci $Y$ v $\phi$, značenou $\sum_Y \phi$, jako faktor $\psi$ s doménou $\vec{X}$ takový, že
\begin{equation*}
    \psi(\vec{X}) = \sum_Y \phi(\vec{X}, Y)
\end{equation*}
Této operaci také říkáme vysčítání $Y$ ve $\phi$.
\end{definice}

Faktory se při počítání chovají jako čísla, všechny operace probíhají po prvcích, pro které je ohodnocení náhodných proměnných z průniku domén faktorů stejné.
Proto platí komutativita $\phi_1 \cdot \phi_2 = \phi_2 \cdot \phi_1$ a $\sum_X \sum_Y \phi = \sum_Y \sum_X \phi$.
Dále platí asociativita součinu $(\phi_1 \cdot \phi_2) \cdot \phi_3 = \phi_1 \cdot (\phi_2 \cdot \phi_3)$.
Nakonec můžeme vyměnit sumu a součin, pokud $X \not\in Dom(\phi_1)$, potom $\sum_X (\phi_1 \cdot \phi_2) = \phi_1 \sum_X \phi_2$.

Sdruženou pravděpodobnost z minulého příkladu tedy můžeme přepsat do formy faktorů.
\begin{equation}
    P(A, B, C, D) = \phi_A \cdot \phi_B \cdot \phi_C \cdot \phi_D
\end{equation}

Opět se pokusíme spočítat marginální pravděpodobnost proměnné $D$.
\begin{align}
P(D) &= \sum_C \sum_B \sum_A \phi_A \phi_B \phi_C
\\
&= \sum_C 
	\phi_D \cdot 
	\left( 
		\sum_B 
			\phi_C \cdot 
			\left( 
				\sum_A 
					\phi_A \cdot \phi_B
			\right)
	\right)
\end{align}

Přesuny sum můžeme provést díky doméně jednotlivých faktorů.
Faktory $\phi_C$ a $\phi_D$ neobsahují proměnnou $A$ a tedy je můžeme vytknout před sumu přes $A$.
Stejně tak faktor $\phi_D$ neobsahuje proměnnou $B$ a opět jej můžeme vytknout před sumu přes $B$.
Tyto úpravy můžeme provádět v libovolném pořadí, pokud vždy platí, že vysčítáme proměnnou $X$ až poté, co spolu vynásobíme všechny faktory, které ji obsahují.

V obecnosti vždy počítáme výraz, který je ve tvaru
\[
\sum_X \prod_{\phi \in \Phi} \phi.
\]

Z tohoto také vychází název pro tuto metodu: sum-product.
Jednoduchý algoritmus pro exaktní inferenci využívající tuto metodu se nazývá eliminace proměnných.
Základní myšlenka je, že máme dán seznam náhodných proměnných v pořadí, v jakém se mají eliminovat.
Pro eliminaci proměnné je třeba nejprve vynásobit všechny faktory, které ji obsahují a následně ji vysčítat.
Tak získáme faktor, který už tuto proměnnou neobsahuje, tedy jsme ji eliminovali.
Eliminace proměnných je popsána v algoritmu~\ref{alg:ve}.

\begin{algorithm}[H]
\caption{Eliminace proměnných}
\label{alg:ve}
\begin{algorithmic}
\Function{Sum-Product-VE}{$\Phi$, $\vec{X}$, $\prec$}
\State{$\Phi$ množina všech faktorů.} 
\State{$\vec{X}$ množina náhodných proměnných, které mají být eliminovaný.}
\State{$\prec$ pořadí proměnných, v jakém mají být eliminovány.}
\State

\State Nechť $X_1, \dots, X_k$ je seřazení proměnných z $\vec{X}$, t.ž. $X_i \prec X_j \Leftrightarrow i < j$.
\For{$i = 1 \dots k$}
	\State $\Phi \gets$ Sum-Product-Eliminate-Var($\Phi$, $Z_i$)
\EndFor
\State $\phi^* \gets \prod_{\phi \in \Phi} \phi$
\State \Return $\phi^*$
\EndFunction
\State
\Function{Sum-Product-Eliminate-Var}{$\Phi$, $X$}
\State{$\Phi$ množina všech faktorů.}
\State{$X$, proměnná, která má být eliminována.}
\State

\State $\Phi^\prime \gets \{\phi \in \Phi: X \in Dom(\phi)\}$
\State $\Phi^{\prime \prime} \gets \Phi - \Phi^\prime$
\State $\psi \gets \prod_{\phi \in \Phi^\prime} \phi$
\State $\tau \gets \sum_X \psi$
\State \Return $\Psi^{\prime\prime} \bigcup \{\tau\}$
\EndFunction
\end{algorithmic}
\end{algorithm}

\begin{theorem}
Nechť $\vec{X}$ je množina náhodných proměnných, nechť $\Phi$ je množina faktorů, t.ž. pro každé $\phi \in \Phi$, $Dom(\phi) \subseteq \vec{X}$.
Nechť $\vec{Y} \subset \vec{X}$ je množina dotazovaných náhodných proměnných a nechť $\vec{Z} = \vec{X} - \vec{Y}$.
Pak pro každé seřazení $\prec$ nad $\vec{Z}$, Sum-Product-VE($\Phi$, $\vec{Z}$, $\prec$) vrátí faktor $\phi^*(\vec{Y})$ takový, že
\[
\phi^*(\vec{Y}) = \sum_Z \prod_{\phi \in \Phi} \phi
\]
\end{theorem}

Nyní provedeme analýzu algoritmu eliminace proměnných.
Předpokládejme, že na vstupu je $n$ proměnných.
Bayesovská síť obsahuje pro každou proměnnou jeden faktor.
Pro jednoduchost budeme předpokládat, že algoritmus bude eliminovat všechny proměnné.
Běh algoritmu se skládá z jednotlivých eliminačních kroků, při kterých je vždy eliminována jedna proměnná.

Při jednom eliminačním kroku je vybrána proměnná $X_i$, všechny faktory, které ji obsahují jsou pronásobeny a vytvoří jeden velký faktor $\psi_i$, proměnná $X_i$ je pak vysčítána z tohoto faktoru.
Počet operací pro jeden eliminační krok tedy závisí na velikosti faktoru $\psi_i$, označme ji $N_i$.
Maximum z velikostí faktorů označme $N_max = max_i N_i$.

Nyní se zaměříme na počet násobení.
Celkem vznikne $n+m$ faktorů, kde $m$ je počet faktorů, které vznikly vysčítáním proměnné.
Každý z těchto faktorů je zahrnut do součinu pouze jednou, pokud je eliminována nějaká proměnná, kterou obsahuje.
Cena násobení faktorů pro vznik $\psi_i$ je nejvýše $N_i$.
Celkový počet násobení tedy bude nejvýše $(n+m)N_{max}$ což je $\mathcal{O}(nN_{max})$.

Pokud $k$ je maximum z velikostí domén proměnných, pak velikost faktoru obsahujícího $n$ proměnných může být až $k^n$.
Složitost eliminace proměnných je tedy dominována velikostí faktorů, které vznikají při výpočtu a je exponenciální.
Navíc bylo dokázáno, že výběr nejlepšího pořadí proměnných pro eliminaci je NP těžký.

\subsection{Aproximativní inference}
