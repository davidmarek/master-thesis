% !TEX root = prace.tex
\chapter{Implementace}
V předchozích kapitolách byly popsány teoretické základy nutné pro implementaci inference v bayesovských sítích.
V této kapitole bude popsána vytvořená knihovna pro dialogové systémy.
Celá knihovna se skládá z několika vrstev, každá z nich stojí na předchozí.
Nejnižší vrstva implementuje efektivní počítání s faktory.
Nad ní stojí vrstva reprezentující jednotlivé vrcholy ve faktor grafu.
Tato vrstva také obsahuje funkcionalitu pro počítání zpráv.
Nejvyšší vrstva se zaměřuje na samotnou propagaci zpráv.

\section{Diskrétní faktor}

Faktor je základním stavebním kamenem.
Operace s faktory musí být efektivní, pro výpočet jedné zprávy je potřeba několik násobení faktorů, následně je třeba je marginalizovat atd.
Faktory je třeba úsporně reprezentovat, každá zpráva, každé pravděpodobnostní rozhraní je samo o sobě faktor.
Faktory jsou reprezentovány třídou \texttt{Factor}, která podporuje všechny základní matematické operace a také řadu specifických operací jako je marginalizace, normalizace, nastavení faktoru na pozorovanou hodnotu, vybrání nejpravděpodobnějšího přiřazení, atd.
Dokumentace třídy je dostupná v příloze \ref{sec:factor}. \todo{Odkaz do dokumentace}

\subsection{Reprezentace faktorů}
\label{sec:repfak}

Každý diskrétní faktor má seznam diskrétních proměnných, které tvoří jeho doménu.
Každá z těchto proměnných může mít jinou kardinalitu, některé proměnné jsou binární, jiné mají mnohem více hodnot.
Faktor je tedy ve své podstatě multidimenzionální tabulka.
Pro implementaci je tato tabulka zploštěná do jednoduchého pole.

Knihovna je napsaná v Pythonu a pro matematické operace používá knihovnu Numpy~\cite{oliphant-2006-guide}.
Pole pak využíváji implementaci z knihovny Numpy, díky které jsou matematické operace napsané v rychlejším jazyku (C, Fortran) než je Python a jsou navíc vektorizované.

Jak je tedy možné reprezentovat multidimenzionální tabulku jednodimenzionálním polem?
Proměnné jsou seřazené a pro zjednodušení můžeme předpokládat, že hodnoty proměnných jsou čísla z $\{0, \dots, n-1\}$, kde $n$ je kardinalita proměnné.
Každá hodnota v poli je pak hodnotou faktoru pro nějaké přiřazení hodnot proměnným, např. $(0, 1, 0)$.
Jednotlivé hodnoty jsou v poli seřazeny lexikograficky.
Pro každou proměnnou si pamatujeme její kardinalitu a také tzv. krok.
Krok určuje kolik pro každou proměnnou o kolik hodnot v tabulce se musíme posunout, abychom se dostali na další hodnotu této proměnné se zachováním hodnot všech následujících.

Příklad faktoru je v tabulce \ref{tab:stride}.
Doménu faktoru tvoří dvě binární proměnné $X$ a $Y$, jejich kardinalita je tedy 2.
Pro proměnnou $X$ je krok 2, pro proměnnou $Y$ je krok 1.

\begin{table}
\begin{center}
\begin{tabular}{|c|c|r|}
\hline
$X$ & $Y$ & Hodnota \\
\hline
\hline
0 & 0 & 0.2 \\
\hline
0 & 1 & 0.3 \\
\hline
1 & 0 & 0.1 \\
\hline
1 & 1 & 0.4 \\
\hline
\end{tabular}
\end{center}
\caption{Příklad faktoru s dvěma proměnnými $X$ a $Y$.}
\label{tab:stride}
\end{table}

\subsection{Operace s faktory}

Implementace diskrétního faktoru obsahuje všechny základní matematické operace, ale také speciální operace, které jsou využity specificky pro pravděpodobnostní rozložení, např. marginalizace anebo normalizace.
Operace jako násobení a marginalizace jsou používány při každém výpočtu zprávy a tedy je třeba je napsat tak, aby fungovaly co nejefektivněji.

Operace s faktory musí fungovat ve třech různých situacích, příklady uvedeme na násobení.
\begin{enumerate}
    \item Násobení faktoru s faktorem, oba se stejnou doménou,
    \item násobení dvou faktorů, které sdíli jen některé proměnné,
    \item násobení faktoru konstantou.
\end{enumerate}

Násobení faktoru konstantou je triviální, každá položka faktoru bude vynásobena konstantou.
Tato operace může být jednoduše vektorizována.

Při násobení dvou faktorů se stejnou doménou je třeba pronásobit prvky se stejným přiřazením proměnných.
Což znamená pronásobit hodnoty na stejných místech v poli.
Opět se tedy jedná o operaci, která je jednoduše vektorizovatelná.

\subsubsection{Operace s různými doménami}

Poslední možnost je, že se snažíme provést matematickou operaci s dvěma faktory, které ovšem nemají stejnou doménu.
Pak musí výsledkem být nový faktor, jehož doména je sjednocením domén vstupních faktorů.
Jednotlivé prvky nového faktoru jsou pak výsledkem aplikace operace na prvky ze vstupních faktorů, které sdílí ohodnocení společných proměnných. 
Příklad s násobením:
\begin{equation*}
\begin{array}{|c|c|r|}
    \hline
    \multicolumn{3}{|c|}{f_1} \\
    \hline
    X & Y & Hodnota \\
    \hline
    \hline
    0 & 0 & 0.2 \\
    \hline
    0 & 1 & 0.3 \\
    \hline
    1 & 0 & 0.1 \\
    \hline
    1 & 1 & 0.4 \\
    \hline
\end{array}
\times
\begin{array}{|c|c|r|}
    \hline
    \multicolumn{3}{|c|}{f_2} \\
    \hline
    Y & Z & Hodnota \\
    \hline
    \hline
    0 & 0 & 0.2 \\
    \hline
    0 & 1 & 0.2 \\
    \hline
    1 & 0 & 0.2 \\
    \hline
    1 & 1 & 0.4 \\
    \hline
\end{array}
=
\begin{array}{|c|c|c|r|}
    \hline
    \multicolumn{4}{|c|}{f_r} \\
    \hline
    X & Y & Z & Hodnota \\
    \hline
    \hline
    0 & 0 & 0 & 0.04 \\
    \hline
    0 & 0 & 1 & 0.04 \\
    \hline
    0 & 1 & 0 & 0.06 \\
    \hline
    0 & 1 & 1 & 0.12 \\
    \hline
    1 & 0 & 0 & 0.02 \\
    \hline
    1 & 0 & 1 & 0.02 \\
    \hline
    1 & 1 & 0 & 0.08 \\
    \hline
    1 & 1 & 1 & 0.16\\
    \hline
\end{array}
\end{equation*}

Výsledek násobení faktorů $f_1$ a $f_2$ je ve faktoru $f_r$.
Faktory sdílí pouze proměnnou $Y$, takže je potřeba pronásobit všechny přiřazení z $f_1$ se všemi přiřazeními z $f_2$, které mají stejnou hodnotu $Y$.
Příkladem je například přiřazení $(0, 1)$ s hodnotou $0.3$ vynásobené s hodnotou $(1, 1)$ s hodnotou $0.4$.
Výsledek je uložen ve faktoru $f_r$ s přiřazením $(0, 1, 1)$ a správnou hodnotou $0.3 \cdot 0.4 = 0.12$.

\subsection{Algoritmus pro operace s různými doménami}

Předvedli jsme možné případy operací s faktory a ukázali, že dva ze tří jsou triviální na implementaci.
Nyní představíme efektivní implementaci třetí možnosti, tedy aplikace operace na dva faktory s různými doménami (algoritmus \ref{alg:apop}).

Ze vstupních faktorů vytvoříme prázdný faktor pro výsledek.
Jeho doména je sjednocením domén vstupních faktorů.
Kardinalita proměnných zůstává stejná.
Krok jednotlivých proměnných je třeba přepočítat.
Spočítáme jej jako součin kardinalit proměnných, které následují po té aktuální.
Velikost pole pro všechny hodnoty je rovna součin všech kardinalit.

Následně přistoupíme k vyplňování tabulky.
Pro oba vstupní faktory si budeme udržovat index na pozici s ohodnocením proměnných, které odpovídá aktuálně vyplňovanému ohodnocení ve výsledném faktoru.
Po provedení operace tyto indexy aktualizujeme.

Pokud reprezentujeme ohodnocení proměnných jako číslo (kde každá cifra může mít jinou kardinalitu), pak se přesuneme k dalšímu ohodnocení v řadě tak, že zvýšíme nejméně signifikantní cifru (proměnnou) o jedna.
Může se stát, že jsme dosáhli kardinality dané proměnné a pak se musíme vrátit na ohodnocení 0 a aplikovat přesun na vyšší cifru.
Opakovanou aplikací přesunu můžeme upravit všechny proměnné, příkladem je přechod z ohodnocení $(0, 1, 1)$ na $(1, 0, 0)$, všechny proměnné binární.
Při každé úpravě proměnné také aktualizujeme indexy ve vstupních faktorech.

Pokud se přesunujeme na další hodnotu proměnné, tak stačí k indexu pro faktor přičíst krok upravené proměnné.
Pokud je třeba se vrátit na ohodnocení 0, pak od indexu pro vstupní faktor odečteme $(c_v - 1) \cdot s_v$, kde $c_v$ je kardinalita proměnné $v$ a $s_v$ je krok proměnné $v$.
Pokud proměnná není obsažena v doméně faktoru, pak její krok je roven 0.

\begin{algorithm}
\caption{Aplikace operace na faktory s různými doménami}
\label{alg:apop}
\begin{algorithmic}
\Function{Apply-Op}{$f_1$, $f_2$, $op$}
\State $f_1, f_2$ -- vstupní faktory
\State $op$ -- operace
\State
\State $f_r \gets$ nový faktor pro výsledek operace na $f_1$ a $f_2$ s prázdným polem
\State
\State $\text{index}[f_1] \gets 0$
\State $\text{index}[f_2] \gets 0$
\State $\text{přiřazení}[v] \gets 0 \text{ pro každou proměnnou } v \in \text{proměnné}[f_r]$
\State
\For{$i \in \{0, \dots, \textsc{Length}(\text{pole}[f_r])-1\}$}
	\State $\text{pole}[f_r][i] = op(\text{pole}[f_1][\text{index}[f_1]], \text{pole}[f_2][\text{index}[f_2]])$
	\For{$v \in \textsc{Reversed}(\text{proměnné}[f_r])$}
		\State $\text{přiřazení}[v] \mathrel{+}= 1$
		\If{\text{přiřazení}[v] = \text{kardinalita}[v]}
			\State $\text{přiřazení}[v] \gets 0$
			\State $\text{index}[f_1] \mathrel{-}= (\text{kardinalita}[v] - 1) \cdot \text{krok}[f_1][v]$
			\State $\text{index}[f_2] \mathrel{-}= (\text{kardinalita}[v] - 1) \cdot \text{krok}[f_2][v]$
		\Else
			\State $\text{index}[f_1] \mathrel{+}= \text{krok}[f_1][v]$
			\State $\text{index}[f_2] \mathrel{+}= \text{krok}[f_2][v]$
			\State \textbf{break}
		\EndIf
	\EndFor
\EndFor
\State \Return $f_r$
\EndFunction
\end{algorithmic}
\end{algorithm}

\subsection{Marginalizace proměnných}

Další důležitou operací často používanou při počítání s faktory je marginalizace.
Na vstupu je faktor a podmnožina proměnných z domény faktoru, které mají zůstat pro vysčítání zbytku.
Algoritmus je podobný aplikaci matematické operace z předchozí sekce.
V tomto případě procházíme původní faktor a každou hodnotu přičteme na správnou pozici v novém faktoru.
Pro aktualizaci indexu v tomto případě musíme projít všechny proměnné, které mají zůstat a u každé zkontrolovat, zda-li se v následujícím kroku změní.

\begin{algorithm}
\caption{Marginalizace faktoru}
\label{alg:marg}
\begin{algorithmic}
\Function{Marginalize}{$f$, $vars$}
\State $f$ -- vstupní faktor, pole incializované na 0
\State $vars$ -- seznam proměnných, které mají zůstat
\State
\State $f_r \gets$ nový faktor, obsahující pouze proměnné z $vars$
\State $\text{přiřazení}[v] \gets 0 \text{ pro každou proměnnou } v \in vars$
\State $index \gets 0$ \Comment{Index do nového faktoru}
\State
\For{$i \in \{0, \dots, \textsc{Length}(\text{pole}[f])-1\}$}
	\State $\text{pole}[f_r][index] \mathrel{+}= \text{pole}[f][i]$
	\State
	\For{$v \in vars$}
		\If{$(i + 1) \bmod \text{krok}[f][v] = 0$}
			\State $\text{přiřazení}[v] \mathrel{+}= 1$
			\State $index \mathrel{+}= \text{krok}[f_r][v]$
		\EndIf
		\State
		\If{$\text{přiřazení}[v] = \text{kardinalita}[v]$}
			\State $\text{přiřazení}[v] \gets 0$
			\State $index \mathrel{-}= \text{kardinalita}[v] \cdot \text{krok}[f_r][v]$
		\EndIf
	\EndFor
\EndFor
\State \Return $f_r$
\EndFunction
\end{algorithmic}
\end{algorithm}

\section{Vrcholy faktor grafu}

Předchozí sekce hovořila o práci s faktory, tato sekce se bude zabývat implementací jednotlivých vrcholů ve faktor grafu.
Vrcholy se dělí na vrcholy s proměnnými a vrcholy pro faktory.
Zde může být názvosloví matoucí, protože implementace faktorů z předchozí sekce se používá pro oboje.
Vrchol pro proměnnou reprezentuje marginální distribuci proměnné.
Vrcholy pro faktory reprezentují pouze samotné faktory, z kterých se skládá sdružená distribuce.
Vrcholy slouží k vytvoření reprezentace grafického modelu a obsahují metody pro výpočet a posílání zpráv.
Implementace vrcholů je v modulu \texttt{node.py} a jejich dokumentace je v příloze \ref{sec:node}. \todo{Odkaz na dokumentaci}

\subsection{Rozhraní vrcholů}

Základní funkcionalita vrcholů je popsána v abstraktní třídě \texttt{Node}.
Vlastností všech vrcholů je jejich sdružování do sítě.
K tomu slouží metoda \texttt{connect}, kterou musí obsahovat každá implementace vrcholu a ta informuje oba vrcholy, že spolu sousedí.
Většina vrcholů si pamatuje své sousedy pro počítání zpráv.
Při připojování proměnných k faktorům je také možné označit proměnnou za rodiče v daném faktoru.
Tato informace je důležitá při normalizaci a učení parametrů.

Dále třída obsahuje metody sloužící pro posílání zpráv.
Nejdůležitější jsou metody \texttt{message\_to} a \texttt{message\_from}.
Vrchol spočítá v metodě \texttt{message\_to} zprávu pro svého souseda.
Má přístup ke všem ostatním vrcholům a tak pro něj není problém zprávu spočítat.
Sousední vrchol zprávu přijme tak, že bude zavolána jeho metoda \texttt{message\_from} a v ní mu bude zpráva předána.
Díky tomu, že na posílání zpráv se podílí odesílatel i příjemce, můžeme kombinovat různé vrcholy, stačí pokud se dohodnout na stejném formátu zprávy.
Dochází tak k oddělení odesílatele od příjemce.

Další metodou, kterou obsahuje každý vrchol je metoda pro inicializaci zpráv \texttt{init\_messages}. 
Pro stávající implementace vrcholů není třeba ji volat před posíláním zpráv, protože zprávy mezi sousedními vrhcoly jsou inicializovány vždy při zavolání metody \texttt{connect} pro jejich propojení.
Pokud ovšem budeme provádět více výpočtů nad jedním grafickým modelem, tak je třeba zprávy nastavit na původní hodnoty před dalším výpočtem, jinak by hodnota zpráv z minulého výpočtu ovlivnila ten následující.

Z optimalizačních důvodů se při odesílání zpráv nepočítá součin všech příchozích vždy znovu, ale je předpočítán a pouze se aktualizuje před odesláním zpráv.
Pro aktualizaci slouží metoda \texttt{update}.
Vzhledem k tomu, že při inferenci je každý faktor vybrán jen jednou a pak jsou z něj odeslány zprávy do všech ostatních vrcholů, stačí metodu \texttt{update} volat pro každý faktor v každé iteraci jen jednou.
Pro odeslání zpráv do všech vrcholů slouží jako zkratka metoda \texttt{send\_messages}.

Implementace vrcholů pro diskrétní proměnné je v třídě \texttt{DiscreteVariableNode}.
Implementace vrcholů pro faktory s diskrétními proměnnými je ve třídě \texttt{DiscreteFactorNode}. 

\subsection{Rozhraní vrcholů pro proměnné}

Smyslem vrcholů pro proměnné je reprezentovat aposteriorní marginální pravděpodobnost proměnných.
Mají navíc několik metod, které jsou potřeba pro výpočty.
Nejprve je třeba mít možnost nastavit pozorované proměnné, k tomu slouží metoda \texttt{observed}.
Tato metoda může být zavolána se slovníkem, kde klíčem jsou pozorovaná přiřazení a hodnotou je pravděpodobnost pozorování.
S tímto pozorováním se pak počítá při posílání zpráv.

Po ukončení výpočtu je třeba zjistit, která hodnota nebo více hodnot patří mezi nejpravděpodobnější.
Některé dialogové strategie totiž potřebují jen jednu nejpravděpodobnější hodnotu, ale jiné mohou počítat se seznamem nejpravděpodobnějších možností. 
K tomu slouží metoda \texttt{most\_probable}.
Vrátí seznam nejpravděpodobnějších přiřazení a jejich pravděpodobností.

\section{Vrcholy pro Dirichletovské parametry}

V kapitole \ref{ch:ep} jsme ukázali jakým způsobem je možné pro diskrétní proměnné učit parametry.
V modulu \texttt{node.py} jsou implementované vrcholy pro práci s dirichletovskými parametry.
Jmenují se \texttt{DirichletParameterNode} a \texttt{DirichletFactorNode}.
V rozhraní se neliší od standardních implementací vrcholů.

\texttt{DirichletParameterNode} musí být vždy spojen pouze s \texttt{DirichletFactorNode}, který už ale může být dále propojen se standardními vrcholy pro diskrétní proměnné.
Díky tomu, že tyto vrcholy o své existenci navzájem ví, tak je možné použít standardní rozhraní pro posílání zpráv.
Při připojení vrcholu pro parametr k faktor vrcholu faktor tento pozná, že se k němu připojil speciální vrchol a bude se tak k němu chovat.
Zprávy od něj bude interpretovat jako parametry pro distribuci zbytku proměnných.
Pro zprávu z faktoru do parametru je třeba provést aproximaci pravděpodobnostní distribuce proměnných a z ní odvodit nové parametry.

\section{Inferenční algoritmus}

V této sekci si popíšeme poslední vrstvu implementovanou v knihovně a to vrstvu starající se o inferenci v grafických modelech.
Předpokládáme, že již máme vytvořený graf z vrcholů, které byly představeny v předchozí sekci.
V modulu \texttt{lbp.py} je implementována třída \texttt{LBP}, která implementuje iterativní algoritmus pro aktualizaci grafického modelu a také obsahuje několik implementací strategií pro různé typy grafů.

Pro výpočet v grafickém modelu je nejprve třeba zaregistrovat všechny vrcholy z grafu.
K tomu existuje několik metod, odlišující se podle typu grafu.
Pro dynamické bayesovské sítě je možné přidávat vrcholy po vrstvách.
K tomu slouží metody \texttt{add\_layer} a \texttt{add\_layers}, první metoda přidá jednu vrstvu na konec sítě.
Druhá metoda umožňuje přidat více vrstev naráz.
Pro obecné grafy slouží metoda \texttt{add\_nodes}, která umožňuje přidat vrcholy bez informace o jejich organizaci uvnitř sítě.

K mazání vrcholů ze sítě slouží metody \texttt{clear\_nodes} a \texttt{clear\_layers}.

Typ inference je možné zadat při vytváření třídy, patřičná strategie je pak použita při samotném výpočtu uvnitř metody \texttt{run}. 
Tato metoda pak provádí samotnou inferenci, je možné zadat počet iterací, v případě inference v dynamickém modelu i zadat od které vrstvy se má inference provádět.

Pro více výpočtů nad jedním grafickým modelem je možné místo inicializace zpráv pro každý vrchol zvlášť použít metodu \texttt{init\_messages}, která tuto inicializaci provede u všech vrcholů.

\subsection{Strategie výběru vrcholu v LBP algoritmu}
\label{sec:noch}

Inference v grafu se může lišit podle typu faktor grafu, ale také podle nároků, které na výsledek máme.
Nejjednodušší metodou pro výběr je nechat pořadí na uživateli algoritmu.
Pro stromy máme speciální strategii, která zaručí konvergenci po jednom dopředném a jednom zpětném kroku propagace.
V dialogových systémech je často používána dynamická bayesovská síť a nás zajímá pravděpodobnost proměnných v poslední vrstvě sítě.
V takovém případě můžeme některé zprávy zanedbat, protože už příliš neovlivní proměnné, které nás zajímají.

\subsubsection{Inference na stromě}

Pro efektivní inferenci je třeba si pro každý vrchol pamatovat, kolik mu chybí zpráv, aby mohl jednu sám odeslat.
Pro každý vrchol $v$ s $k$ sousedy je na začátku počet chybějících zpráv $k-1$.
Strom na alespoň dvou vrcholech obsahuje alespoň dva listy.
Budeme tedy postupně odebírat vrcholy, jejichž počet chybějících zpráv je nulový.
Rozešleme z nich zprávu do souseda, z kterého ještě zpráva nepřišla a snížíme jeho počet chybějících zpráv.
Odebráním listu ze stromu vždy dostaneme opět strom.
Postupně tak zmenšujeme strom, až dostaneme právě jeden vrchol, který získal všechny zprávy.

Po vypočtení marginální pravděpodobnosti ve stromě o jednom vrcholu můžeme zase přídávat vrcholy v obráceném pořadí než v jakém jsme je odebírali.
Do každého přidaného vrcholu pak můžeme poslat zprávu a spočítat jeho marginální pravděpodobnost.

\subsubsection{Inference v dynamické bayesovské síti}

Dynamická bayesovská síť je nejčastější reprezentace dialogového stavu.
V jedné vrstvě sítě je popsána jedna obrátka dialogu.
Vrcholy v jedné vrstvě většinou závisí pouze na vrcholech ve stejné nebo předchozí vrstvě.
Po každé obrátce nás pro účely dialogového manageru zajímají hlavně pravděpodobnosti proměnných v poslední vrstvě, tedy po aktuální obrátce.

Inferenci provádíme způsobem podobným indukci.
Pro první vrstvu provedeme inferenci libovolným způsobem, může dokonce platit, že v rámci jedné vrstvy se jedná o strom.
Po přidání $k$-té vrstvy předpokládáme, že byla provedena inference na předchozích $k-1$ vrstvách a tedy zprávy v této části grafu jsou správné.
Zprávy ve směru k nové vrstvě jsou stále správné, neznáme ovšem hodnotu zpráv z předposlední vrstvy do nové vrstvy a hodnotu zpráv v nové vrstvě.
Pošleme tedy zprávy z předposlední vrstvy a provedeme inferenci v nové vrstvě, opět můžeme použít libovolnou heuristiku.

Stále ještě zbývají zprávy z nové vrstvy zpět v síti.
Čím dál do historie ovšem jdeme, tím menší vliv naše nová pozorování budou mít na vrcholy v dané vrstvě.
Proto se většinou omezíme jen na posledních několik vrstev (1 až 3).

\section{Příklady}

Po popisu jednotlivých vrstev v předchozí sekci nyní přejdeme k příkladům použití knihovny.
Nejprve ukážeme použití jednotlivých tříd a metod, následně se přesuneme k použití v jednoduchých modelech.
Nakonec bude představen příklad systému použitého v Dialog State Tracking Challenge (DSTC) 2013~\cite{dstc2013}.

\subsection{Použití jednotlivých komponent}

Představíme příklady vytvoření a používání jednotlivých komponent, od implementace faktorů až po inferenci v grafickém modelu.

\subsubsection{Faktor}

Třída \texttt{Faktor} je popsána v sekci \ref{sec:repfak}.
Vytvoření faktoru je ukázáno v kódu~\ref{lst:crfac}.
Vytvořený faktor obsahuje dvě proměnné, první je požadovaný typ jídla, druhá je hypotéza o požadovaném jídle.
Tento faktor může pocházet z generativního modelu, pravděpodobnost pozorování závisí na skutečné hodnotě.
Také je větší pravděpodobnost, že budeme pozorovat typ jídla, který uživatel požaduje.
Pro indické jídlo je větší pravděpodobnost, že budeme pozorovat opravdu to, co uživatel řekl, v reálném dialogovém systému na tyto pravděpodobnosti může mít vliv například jazykový model, anebo podobnost slov.

\begin{lstlisting}[
	caption={Vytvoření faktoru},
	label={lst:crfac}
]
from factor import Factor

factor = Factor(
    ['food', 'food_obs'],
    {
        'food': ['chinese', 'indian'],
        'food_obs': ['obs_chinese', 'obs_indian'],
    },
    {
		('indian', 'obs_indian'): 0.9,
		('indian', 'obs_chinese'): 0.1,
		('chinese', 'obs_indian'): 0.2,
		('chinese', 'obs_chinese'): 0.8,
    })
\end{lstlisting}

Vytvořený faktor můžeme zobrazit \ref{lst:facprt}, stačí použít příkaz \texttt{print}, popřípadě je možné použít metody \texttt{pretty\_print}, která vrátí řetězec s formátovaným výpisem faktoru.
Nabízí možnost nastavení šířky tabulky a počtu desetinných míst.

\begin{lstlisting}[
	caption={Zobrazení faktoru},
	label={lst:facprt},
	float=h,
]
>>> print factor
--------------------------------------------------------------------------
           food                    food_obs                   Value
--------------------------------------------------------------------------
         chinese                 obs_chinese               0.8000000119
         chinese                  obs_indian               0.1999999881
          indian                 obs_chinese              0.09999999404
          indian                  obs_indian               0.8999999762
--------------------------------------------------------------------------
>>> print factor.pretty_print(width=40, precision=2)
----------------------------------------
    food       food_obs       Value
----------------------------------------
   chinese    obs_chinese      0.8
   chinese    obs_indian       0.2
   indian     obs_chinese      0.1
   indian     obs_indian       0.9
----------------------------------------
\end{lstlisting}

Dále obsahuje faktor implementaci matematických operací, je možné je používat se standardními operátory. 
Příklady budou ukázány na násobení~\ref{lst:facmul}.
Vytvoříme další faktor, který reprezentuje přechodovou pravděpodobnost.
V grafickém modelu bychom je zřejmě spolu nenásobili, zde tak provedeme z ilustračních důvodů.
Operace fungují i s konstantami~\ref{lst:facmulk}

\begin{lstlisting}[
	caption={Násobení faktorů},
	label={lst:facmul}
]
>>> factor_trans = Factor(
    ['food', 'food_next'],
    {
        'food': ['chinese', 'indian'],
        'food_next': ['chinese', 'indian'],
    },
    {
		('indian', 'indian'): 0.9,
		('indian', 'chinese'): 0.1,
		('chinese', 'indian'): 0.1,
		('chinese', 'chinese'): 0.9,
    })
>>> result = factor * factor_trans
>>> print result.pretty_print(50, 2)
--------------------------------------------------
    food     food_next    food_obs     Value
--------------------------------------------------
  chinese     chinese   obs_chinese     0.72
  chinese     chinese    obs_indian     0.18
  chinese      indian   obs_chinese     0.08
  chinese      indian    obs_indian     0.02
   indian     chinese   obs_chinese     0.01
   indian     chinese    obs_indian     0.09
   indian      indian   obs_chinese     0.09
   indian      indian    obs_indian     0.81
--------------------------------------------------
\end{lstlisting}

\begin{lstlisting}[
	caption={Násobení faktorů konstantou},
	label={lst:facmulk},
	float=h,
]
>>> result = factor * 0.5
>>> print result.pretty_print(50, 2)
--------------------------------------------------
      food          food_obs         Value
--------------------------------------------------
    chinese       obs_chinese         0.4
    chinese        obs_indian         0.1
     indian       obs_chinese         0.05
     indian        obs_indian         0.45
--------------------------------------------------
\end{lstlisting}

Další důležitou metodou, kterou faktory nabízí je marginalizace proměnných.
Předvedeme si ji na faktoru pro pozorování~\ref{lst:facmar}, pokud bychom chtěli získat pouze pravděpodobnosti pozorování.
\begin{lstlisting}[
	caption={Marginalizace faktoru},
	label={lst:facmar},
	float=h,
]
>>> marginalized = factor.marginalize(['food_obs'])
>>> print marginalized.pretty_print(50, 2)
--------------------------------------------------
        food_obs                   Value
--------------------------------------------------
       obs_chinese                  0.9
       obs_indian                   1.1
--------------------------------------------------
\end{lstlisting}

Pokud je faktor použit pro reprezentaci pravděpodobnostního rozdělení, pak je možné, že při některých úkonech bude výsledkem nenormalizované pravděpodobnostní rozdělení.
Faktor nabízí metodu pro normalizaci hodnot, která navíc bere v úvahu i podmíněné pravděpodobnosti~\ref{lst:facnor}

\begin{lstlisting}[
	caption={Normalizace faktoru},
	label={lst:facnor}
]
>>> uniform = Factor(
    ['food', 'food_next'],
    {
        'food': ['chinese', 'indian'],
        'food_next': ['chinese', 'indian'],
    },
    {
		('indian', 'indian'): 1,
		('indian', 'chinese'): 1,
		('chinese', 'indian'): 2,
		('chinese', 'chinese'): 2,
    })
>>> uniform.normalize(parents=['food'])
>>> print uniform.pretty_print(50, 2)
--------------------------------------------------
      food         food_next         Value
--------------------------------------------------
    chinese         chinese           0.5
    chinese          indian           0.5
     indian         chinese           0.5
     indian          indian           0.5
--------------------------------------------------
\end{lstlisting}

Nakonec faktor nabízí možnost zjistit, které ohodnocení jsou nejpravděpodobnější~\ref{lst:facmop}.
\begin{lstlisting}[
	caption={Nejpravděpodobnější hodnoty},
	label={lst:facmop}
]
>>> food = Factor(
    ['food'],
    {
        'food': ['chinese', 'indian', 'french'],
    },
    {
		('chinese',): 0.3,
		('indian',): 0.6,
		('french',): 0.1,
    })
>>> food.most_probable(n=2)
[('indian', 0.6), ('chinese', 0.3)]
\end{lstlisting}

\subsubsection{Vrcholy}

V této sekci ukážeme vytváření jednoduchého grafického modelu a následně poslání zpráv z jednoho vrcholu do druhého.
V zdrojovém kódu~\ref{lst:nodex} vytvoříme jednoduchý skrytý markovský model pro výběr jídla.
Model bude mít 2 obrátky, v každé bude jedna skrytá a jedna pozorovaná proměnná.
Tento model bude generativní, bude obsahovat faktor pro generování pozorování na základě skutečné hodnoty a také faktor pro přechodovou pravděpodobnost z jedné obrátky do druhé.

\begin{lstlisting}[
	caption={Jednoduchý generativní model},
	label={lst:nodex},
	float=h
]
from alex.ml.bn.node import DiscreteVariableNode, DiscreteFactorNode
from alex.ml.bn.factor import Factor

hid_1 = DiscreteVariableNode('food_1', ['chinese', 'indian'])
obs_1 = DiscreteVariableNode('food_obs_1', ['chinese', 'indian'])
obs_factor_1 = DiscreteFactorNode('food_obs_factor_1', Factor(
    ['food_1', 'food_obs_1'],
    {
        'food_1': ['chinese', 'indian'],
        'food_obs_1': ['chinese', 'indian'],
    },
    {
        ('chinese', 'chinese'): 0.9,
        ('chinese', 'indian'): 0.1,
        ('indian', 'chinese'): 0.1,
        ('indian', 'indian'): 0.9,
    }))

hid_2 = DiscreteVariableNode('food_2', ['chinese', 'indian'])
obs_2 = DiscreteVariableNode('food_obs_2', ['chinese', 'indian'])
obs_factor_2 = DiscreteFactorNode('food_obs_factor_2', Factor(
    ['food_2', 'food_obs_2'],
    {
        'food_2': ['chinese', 'indian'],
        'food_obs_2': ['chinese', 'indian'],
    },
    {
        ('chinese', 'chinese'): 0.9,
        ('chinese', 'indian'): 0.1,
        ('indian', 'chinese'): 0.1,
        ('indian', 'indian'): 0.9,
    }))

trans_factor = DiscreteFactorNode('food_trans_factor', Factor(
    ['food_1', 'food_2'],
    {
        'food_1': ['chinese', 'indian'],
        'food_2': ['chinese', 'indian'],
    },
    {
        ('chinese', 'chinese'): 0.99,
        ('chinese', 'indian'): 0.01,
        ('indian', 'chinese'): 0.01,
        ('indian', 'indian'): 0.99,
    }))

obs_factor_1.connect(hid_1, parent=True)
obs_factor_1.connect(obs_1, parent=False)

obs_factor_2.connect(hid_2, parent=True)
obs_factor_2.connect(obs_2, parent=False)

trans_factor.connect(hid_1, parent=True)
trans_factor.connect(hid_2, parent=False)
\end{lstlisting}

Pokud máme vytvořený grafický model, dalším krokem je nastavit pozorované hodnoty proměnných~\ref{lst:facobs}.
Předpokládejme, že v první obrátce bylo pozorováno čínské jídlo s pravděpodobností $0.6$ a indické s pravděpodobností $0.4$.
V druhé obrátce bylo pozorováno čínské jídlo s pravděpodobností $0.5$ a indické s pravděpodobností $0.5$.
Tedy, v první obrátce si myslíme, že uživatel spíše preferuje čínské jídlo, ale z druhé obrátky už nevíme nic. 
Podíváme se, co z těchto informací zjistí náš grafický model.

\begin{lstlisting}[
	caption={Nastavení pozorovaných hodnot},
	label={lst:facobs},
	float=h
]
obs_1.observed({
	('chinese',): 0.6,
	('indian',): 0.4,
})

obs_2.observed({
	('chinese',): 0.5,
	('indian',): 0.5,
})
\end{lstlisting}

Strom zakořeníme ve vrcholu \texttt{trans\_factor} a začneme posílat zprávy od listů ke kořeni a pak zpátky~\ref{lst:facmsg}.
Naším cílem je zjistit aposteriorní marginální pravděpodobnosti skrytých proměnných.
Před odesláním je vždy třeba aktualizovat vnitřní reprezentaci příchozích zpráv.
Nakonec musíme normalizovat vrcholy s proměnnými.

\begin{lstlisting}[
	caption={Posílání zpráv},
	label={lst:facmsg},
	float=h
]
obs_1.message_to(obs_factor_1)
obs_2.message_to(obs_factor_2)

obs_factor_1.update()
obs_factor_1.message_to(hid_1)

obs_factor_2.update()
obs_factor_2.message_to(hid_2)

hid_1.update()
hid_1.message_to(trans_factor)

hid_2.update()
hid_2.message_to(trans_factor)

trans_factor.update()
trans_factor.send_messages()

hid_1.update()
hid_1.normalize()

hid_2.update()
hid_2.normalize()
\end{lstlisting}

\begin{lstlisting}[
	caption={Výsledek inference},
	label={lst:facres},
	float=h
]
>>> print hid_1.belief.pretty_print(50, 2)
--------------------------------------------------
         food_1                    Value          
--------------------------------------------------
         chinese                   0.58           
         indian                    0.42           
--------------------------------------------------
>>> print hid_2.belief.pretty_print(50, 2)
--------------------------------------------------
         food_2                    Value          
--------------------------------------------------
         chinese                   0.58           
         indian                    0.42           
--------------------------------------------------
\end{lstlisting}

Ve výsledku~\ref{lst:facres} tedy vidíme, že v druhé obrátce z pozorování nezískáme žádnou informaci, ale díky vysoké pravděpodobnosti přechodu zůstala pravděpodobnost čínského jídla stále vysoká.
Toto pozorování zároveň snížilo pravděpodobnost v první obrátce.

\subsubsection{Inference}

Nyní se zbavíme manuálního posílání zpráv z~\ref{lst:facmsg} a nahradíme jej použitím třídy pro Loopy Belief Propagation~\ref{lst:lbpex}. 
Zvolíme strategii pro stromy a výsledek bude stejný jako v případě s manuálním posíláním zpráv.

\begin{lstlisting}[
	caption={Inference s pomocí LBP},
	label={lst:lbpex},
	float=h
]
from alex.ml.bn.lbp import LBP

lbp = LBP(strategy='tree')
lbp.add_nodes([obs_1, obs_2, hid_1, hid_2, obs_factor_1, obs_factor_2,
               trans_factor])
lbp.run()
\end{lstlisting}

\section{Dialog State Tracking Challenge}

Knihovna pro inferenci byla použita v Dialog State Tracking Challenge (DSTC) 2013~\cite{dstc2013}.
Nejprve popíšeme problém, následně představíme použitý model a nakonec porovnáme výsledky modelu s ostatními systémy.

Cílem DSTC bylo vytvořit prostředky pro porovnání různých přístupů k inferenci dialogového stavu a vyhodnotit jejich úspěšnost pomocí společné množiny metrik.
Organizátoři poskytli několik anotovaných množin dat, které pocházejí z reálného použití tří různých dialogových systémů pro úlohu Let's Go.
Zároveň poskytli i baseline dialogový systém.
Cílem účastníků soutěže bylo vytvořit systém odhadu dialového stavu, který správně predikuje dialogový stav dáno vstup uživatele a minulé akce systému.

\subsection{Let's Go}

Úlohou dialogového systému Let's Go je poskytnout telefonní službu pro zjišťování autobusového spojení v Pittsburghu.
Systém rozpoznává 9 různých slotů, některé z nich sestávající z podslotů.
Tyto sloty jsou
\begin{itemize}
\item \emph{route} -- linka,
\item \emph{time} -- čas,
\item \emph{from.description} -- popis místa odjezdu,
\item \emph{to.description} -- popis místa příjezdu,
\item \emph{from.monument} -- významný monument v místě odjezdu,
\item \emph{to.monument} -- význámný monument v místě příjezdu,
\item \emph{from.neighborhood} -- část města (sousedství) z které chce uživatel jet,
\item \emph{to.neighborhood} -- část města (sousedství) kam chce uživatel jet.
\end{itemize}
Navíc se sloty \emph{time} a \emph{data} skládají z dalších 5, respektive 4 slotů (např. \emph{time.hour}, \emph{time.minute}).
Systém Let's Go celkem rozpoznával více než 5000 míst a zhruba 150 různých linek.

Jednotlivé dialogy se skládají z obrátek, ve kterých se střídá uživatel a systém.
Uživatel může informovat systém o hodnotě libovolného slotu.
Systém se může uživatele požádat o hodnotu libovolného slotu, anebo o potvrzení hodnoty nějakého slotu.
Vstup a výstup je reprezentován dialogovými akty.
Používány jsou pouze dialogové akty \emph{inform}, \emph{deny}, \emph{affirm} a \emph{negate}.
Zbytek dialogových aktů je ignorován, protože nemění cíle uživatele.

Organizátoři poskytli 4 datové sady, které pocházely ze 3 různých dialogových systémů.
Každá datová sada obsahovala hypotézy z živého běhu dialogového systému.
Dva z dialogových systémů produkovala seznam $n$ nejlepších hypotéz.
Jeden systém produkoval pouze nejlepší hypotézu, záznamy hovorů byly zpracovány zpětně pro vygenerování seznamů $n$ nejlepších hypotéz.
Vygenerované hypotézy ovšem obsahovaly pouze skóre, proto jsme se rozhodli je nepoužít pro pravděpodobnostní inferenci.

\subsection{Generativní model}

Dialogový stav byl v použitém systému modelován jednoduchým generativním modelem, kde pro každý slot v jedné obrátce existují dva vrcholy, skutečná hodnota slotu $s_t$ a pozorovaná hodnota $o_t$.  
Stav důvěry skutečné hodnoty slotu $b(s_t)$ závisí pouze na hodnotě v minulé obrátce $s_{t-1}$ a na poslední systémové akci $a_{t-1}$.
Pozorovaná hodnota $o_t$ závisí pouze na skutečné hodnotě.

Stav důvěry můžeme vyjádřit ze sdružené pravděpodobnosti generativního modelu:
\begin{equation}
b(s_t) = \sum_{s_{t-1}, o_t} p(s_t \mid a_{t-1}, s_{t-1}) p(o_t \mid s_t) b(s_{t-1})
\end{equation}

Pro zrychlení výpočtu bylo využito stažených parametrů~\cite{thomson2010bayesian} s manuálně nastavenými hodnotami:
\begin{equation}
p(s_t \mid a_{t-1}, s_{t-1}) = \begin{cases}
\theta_t & \text{pokud } s_t = s_{t-1} \\
\frac{1 - \theta_t}{|hodnoty| - 1} & \text{jinak}
\end{cases},
\end{equation}
kde $\theta_t$ je pravděpodobnost, že hodnota slotu se nemění a $|hodnoty|$ je počet hodnot pro slot.

Pro model pozorování byly také použity stažené pravděpodobnosti:
\begin{equation}
p(o_t \mid s_t) = \begin{cases}
\theta_o & \text{pokud } o_t = s_t \\
\frac{1 - \theta_o}{|hodnoty| - 1} & \text{jinak}
\end{cases},
\end{equation}
kde $\theta_o$ je pravděpodobnost, že pozorování bude odpovídat skutečné hodnotě slotu.

Parametr $\theta_t$ určuje jak moc bude systém zapomínat, v případě hodnoty blízké 1 systém téměř nezapomíná a jakmile nějakou hodnotu pozoruje, bude jej těžké přesvědčit o čemkoliv jiném, v opačném případě naopak systém zapomíná a i z téměř jisté důvěry v hodnotu stavu může během pár obrátek dojít do stavu, kdy mají všechny možné hodnoty stejnou pravděpodobnost.
Parametr $\theta_o$ vyjadřuje důvěru systému v SLU.
Pokud je jeho hodnota vysoká, předpokládáme, že SLU téměř nedělá chyby.
V opačném případě je systém tolerantní k chybám SLU.
Na základě výsledků dialogového systému na trénovacích dat byly manuálně zvoleny vhodné hodnoty parametrů: $\theta_t = 0.8$ a $\theta_o = 0.8$

Představený model pro pozorování platí pro \emph{inform} dialogové akty, dialogové sloty \emph{affirm} a \emph{negate} byly převedeny na \emph{inform}.

Pro inferenci bylo použito LBP, protože možných hodnot slotů bylo v řádu stovek, tak všechny nepozorované hodnoty jsou stažené do jedné speciální hodnoty a jejich pravděpodobnost je počítána pouze dohromady.

\subsection{Evaluace}

Použitý model byl testován na 4 testovacích datových sadách s živými daty a na dvou testovacích datových sadách s doplněnými hypotézami.
Výsledky byly porovnány s baseline dialogovým systémem vytvořeným organizátory.
Použité metriky byly přesnost (accuracy) dialogového systému a Brier score.
Přesnost je spočítána jako procentuální zastoupení obrátek, kdy nejlepší hypotéza systému pro odhad stavu byla ta správná.
Brier score měří přesnost prediktivní pravděpodobnostní distribuce systému pro odhad stavu~\cite{brier1950verification}.